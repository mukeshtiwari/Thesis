\chapter{Theorem Prover and Cryptography}
\label{cha:theorem_crypto}
 \setlength{\parindent}{2em}
\setlength{\parskip}{1em}

\epigraph{All our knowledge begins with the senses, proceeds then to the understanding, and
 ends with reason. There is nothing higher than reason.} 
{\textit{Immanuel Kant}} 

A proof assistant or theorem prover is a computer program which assists users in development 
of mathematical proofs. Basically, the idea of 
developing mathematical proofs using computer goes back to Automath (automating mathematics)
\citep{deBruijn1983} and LCF \citep{Milner:1972:IAS:942578.807067}. The 
Automath project (1967 until the early 80's)  was initiative of De Bruijn, and the aim of the project was to 
develop a language for expressing mathematical theories which can be verified by aid of computer.  
Moreover, the Automath was first 
practical project to exploit the Curry-Howard isomorphism (proofs-as-programs and formulas-as-types). 
DeBruijn  was likely unaware of this correspondence, and he almost re-invented it.
The Automath project can be seen as the precursor of
 proof assistants NuPrl \citep{Constable:1986:IMN:10510} and Coq \citep{Bertot:2004:ITP}.  
 Some other notable  proof assistants are 
 Nqthm/ACl2 \citep{507872}, PVS \citep{Owre:1992:PPV:648230.752639},
 HOL (a family of tools derived from LCF theorem prover) \citep{Slind:2008:BOH:1459784.1459792}
 \citep{Harrison:1996:HLT:646184.682934} \citep{Nipkow:2002:IHP},
 Agda \citep{Norell:2008:DTP:1813347.1813352}, and Lean \citep{10.1007/978-3-319-21401-6_26}.


\textbf{Chapter overview:}
 This chapter is an overview of the Coq theorem prover and cryptographic primitives. 
 In the Section \ref{sec:problemstatement}, we will give a brief overview of 
 theoretical foundation, calculus of construction and calculus of inductive 
 construction, of Coq.  In the Section \ref{sec:typeprop}, we will discuss
 the difference between \texttt{Type} and \texttt{Prop}
 which is very crucial from program extraction point of view.  The goal of 
 our formalization was not only proving the correctness of 
 Schulze method, but extracting  OCaml/Haskell code to count 
 ballots.  In the Section \ref{sec:deplambda}, we will focus 
 on dependent types and  how it leads to correct by construction paradigm
 by designing a  type safe printf function. 
 Section \ref{sec:gallina} focuses on Coq specification language 
 \texttt{Gallina} with an example showing that why writing proofs using  
 \texttt{Gallina} is difficult and cumbersome, and how it can be eased by 
 using tactics. Finally, in the Section  \ref{sec:coqproof}, we will take 
 philosophical route to justify that why we should trust in Coq proofs 
 even though they do not appear anywhere near to a proof written by 
 a human.  In the Section \ref{sec:cryptography}, we give some historical 
 context and modern day usage of cryptography.  In the following Section 
 \ref{sec:group}, we describe \textit{Group} which is the underlying 
 algebraic structure for Diffie-Hellman construction (\ref{sec:diffie-hellman}). 
 In the next two sections, we describe the ElGamal encryption (\ref{sec:elgamal}) 
 and Homomorphic Encryption (\ref{sec:homomorphic-enc}). In addition, 
 we show the both homomorphic property, multiplicative and additive, 
 of ElGamal encryption. We explain the concept of zero-knowledge proof 
 and zero-knowledge proof  of knowledge in Section \ref{sec:zkp}. 
 In the next two sections, we discuss Sigma protocols (\ref{sec:sigma}), 
 an efficient way to achieve zero-knowledge proof, and commitment schemes
 (\ref{sec:commscheme}), a cryptographic protocol to force two mutually 
 distrusting parties to behave honestly with the explanation of Pedersen 
 commitment scheme based on discrete logarithm.  Finally, we give a 
 brief summary pointing to the resources for theorem proving and 
 cryptography. 
 


\section{Coq: Interactive Proof Assistant}
\label{sec:problemstatement}
Coq  is an interactive proof assistant (theorem prover) based on
theory of Calculus of 
Inductive Construction \citep{Paulin-Mohring:1993:IDS:645891.671440} which itself is an 
augmentation of Calculus of Construction 
\citep{Coquand:1988:CC:47724.47725}.  
The underlying theory of CoC and CIC is (typed) lambda calculus, so 
before we describe the CoC and CIC syntax and its typing judgement, 
we will take a brief detour to explain 
different variants of lambda calculus starting from 
untyped lambda calculus and moving up in the ladder by 
adding various abstractions. Later, we will show that 
these all variant, including CoC, can be abstracted into one 
framework by using pure type system \citep{berardi1988towards} 
\citep{Barendregt:1993:LCT:162552.162561}.  
In addition, 
pure type system can be extended with three rules, 
inductive data type, pattern matching, and recursion 
to accommodate Calculus of Inductive Construction.

Lambda calculus was invented by Alonzo Church in the 1930s, 
and his motive was to use lambda calculus as a foundation 
for formal mathematics, specifically the notation of 
computable function by means of an algorithm.  
It is a simplest programming language having just 
three constructs, i.e. variable, application, and abstraction, 
and the abstract syntax tree of lambda calculus is:
\begin{displayquote}

T = V (* Variables *) \\
   | $\lambda$ V. T (* Abstraction *) \\
   | T T       (* Application *)

\end{displayquote}

Using these three rules, we can construct the lambda terms corresponding to 
various mathematical notions. For example, we can represent
$True$ as $\lambda x. \lambda y. x$, $False$ as $\lambda x. \lambda y. y$,
$Zero$ as $\lambda f.\lambda x. x$,  $One$ as $\lambda f.\lambda x. f x$, etc. 
However, there is nothing which is stopping us to construct lambda terms which 
has no apparent meaning, e.g. applying a variable $x$ with itself learning to a lambda 
term $x x$. To avoid these kind of terms, we extend the lambda calculus with 
another abstraction called \textit{types}. Moreover, we add \textit{typing judgement} (rule) 
which dictates which terms is well-typed and which one is not.  This new 
lambda calculus augmented with \textit{types} is known as 
\textit{Simple Typed Lambda Calculus}, represented as $\lambda^{\to}$. 
The abstract syntax tree for \textit{Simple Typed Lambda Calculus} is:

\begin{displayquote}

 
$\mathcal{T} = \mathcal{V} $ (* Type Variable *) \\
                     | $\mathcal{T \rightarrow T}$ (* Arrow Type *)

\end{displayquote}

\begin{displayquote}
T = V (* Variables *) \\
   | $\lambda$ V : $\mathcal{T}$. T (* Abstraction *) \\
   | T T       (* Application *)

\end{displayquote}

\noindent
The typing judgement is a relation between \textit{types} and \textit{terms} in some abstract typing context $\Gamma$. The $\Gamma$ 
is a set/list of typing assumption of the form $x : A$, meaning $x$ is of type $A$. Moreover, $\Gamma$ $\vdash$ $x : A$ means that 
the term $x$ has type $A$ in the context $\Gamma$.
The typing judgement of \textit{Simple Typed Lambda Calculus} 
has three rules, \textit{Var, Abstraction}, and \textit{Application}, to ensure that the terms are well-typed:
\begin{itemize}
\item Var: \[ {\frac {\text{x : A }  \in \text{  }\Gamma }{\Gamma \text{  } \vdash \text{  x : A }} \]
\item Abstraction: \[ {\frac {\Gamma \text{, x : A } \vdash \text{ e : B }}{\Gamma \text{  }\vdash \text{ } (\lambda \text{ x : A. e ) : }A \rightarrow B}} \]
\item Application: \[ {\frac {\Gamma \text{  } \vdash \text{ f : } A \rightarrow B \quad \Gamma \text{  }\vdash \text{ x : A }}{\Gamma \text{  } \vdash \text{  f  x  : B} }} \]
\end{itemize}

\noindent
Now these three typing judgement rules rule out the term $x x$ because it is not well-typed term, and this can be 
inferred from \textit{Application} rule. 
The  \textit{Application} rule states that for $f$ $x$ to be a well typed term of some type $B$, the $f$ has to have 
a type $A \rightarrow B$ for some type $A$ and $x$ has to have the type $A$. Following the 
\textit{Application rule}, for $x x$ to be well typed, the $x$ has to have a arrow type $A \rightarrow B$
and type $A$ in some typing context $\Gamma$. However, it is not possible that $A$ = $A \rightarrow B$
leading to rejection of term $xx$. 

Simple typed lambda calculus is great for many practical purposes except it is verbose. Consider a function 
which takes a input and simply returns it, also known as identity function. Assuming that we two 
base type, $nat$ for the type of natural numbers and $bool$ for type of boolean values, as member of 
type variable set $\mathcal{V}$. We can represent a
 identity function on boolean value as $\lambda x : bool. x$, and on natural number as 
 $\lambda x : nat. x$. In general, we would have one identity function per type. We can 
 abstract these types in to type variable, but we need to type these type variables as well
 to keep everything well typed.  Consequently, abstracting the 
 \textit{types} over type variable, which itself is of sort \textit{kinds} and represented as $*$, 
 leads to \textit{Second Order Lambda Calculus} ($\lambda2$), and 
 now the identity function over different types can be abstracted into a single function:
 $\lambda \alpha : \star. \lambda x : \alpha. x$.
 There are various other variants or abstractions 
 of typed lambda calculus, which we would not discuss here, that can be categorized into:
 \begin{itemize}
 \item Terms depending on terms ($\lambda^{\to}$)
 \item Terms depending on types  ($\lambda2$)
 \item Types depending on types  ($\lambda {\underline{\omega}}$)
 \item Types depending on terms  ($\lambda P$)
 \end{itemize}


% \begin{figure}[!htb]
%        \center{\includegraphics[width=\textwidth]{figs/Lambda_Cube_img.svg.png}}
%        \caption{Lambda Cube}
%      \end{figure} 

All these variation of lambda calculus can be captured into a unified framework
known as \textit{Pure Type System} or \textit{Generalized Type System}.
The \textit{PTS} is group of type system that allows the dependencies between 
types and terms. Unlike the simple typed lambda calculus ($\lambda^{\to}$) where 
terms and types live in two disjoint world, \textit{PTS} blurs this distinction between 
types and terms.  The abstract syntax  of pure type system:
 \begin{displayquote}

    T = V   (* \textit{variable} *)\\
       |  C   (* \textit{constant} *)\\
       | T T (* \textit{application} *)\\
       | $\lambda$ V : T. T (* \textit{abstraction}*) \\
       | $\prod$ V : T. T  (* \textit{dependent function type} *)
   \end{displayquote}

\noindent
The pure type system parametrized by a specification, i.e. 
set of sorts $S$, axioms $A$, and rules $R$, such that: 

 \begin{itemize}
	\item $S$ is a subset of  $C$, i.e.  $S \subseteq C$. 
	\item $A$ is the axioms of form \textit{c : s} where $c \in C$ and $s \in S$, i.e.  $A \subseteq C \times S$. 
	\item $R$ is the set of rules of form $(s_{1}, s_{2}, s_{3})$ such that $s_{1}, s_{2}, \text{ and } s_{3} \in S$, i.e.
	 $R \subseteq S \times S \times S$.
 
 \end{itemize}
 
\noindent 
The typing judgement for \textit{PTS} in typing context $\Gamma$ is defined by following rules ($s$ ranges of $S$, and $x$ ranges over V 
 with usual notion of  variable capture avoidance):
 \begin{itemize}
 \item Axiom: \[ \frac{\text{c : s } \in A}{\Gamma \text{  } \vdash \text{ c : s}} \]
 \item Start: \[ \frac{\Gamma \text{  } \vdash \text{ A : s}}{\Gamma \text{, x : A }\vdash \text{ x : A}} \]
 \item Weakening: \[ \frac{\Gamma \text{ } \vdash \text{ A : B } \quad \Gamma \text{ } \vdash \text{ C : s }}{\Gamma, \text{ x : C } \vdash \text{ A : B }} \]
 \item Product: \[ \frac{\Gamma \text{ } \vdash \text{ A : }s_{1} \quad \Gamma, \text{ x : A } \vdash \text{ B : }s_{2}}{\Gamma \text{ } \vdash (\prod \text{ x : A. B) : }s_{3}} \]
 \item Application: \[ \frac{\Gamma \text{ }\vdash \text{ F : (}\prod \text{x : A. B)} \quad \Gamma \text{ }\vdash \text{ a : A }}{\Gamma \text{ }\vdash \text{ F a : B [x := a]}} \]
 \item Abstraction: \[ \frac{\Gamma, \text{ x : A }\vdash \text{ b : B } \quad \Gamma \text{ }\vdash (\prod \text{x : A. B) : s }}{\Gamma \text{ }\vdash (\lambda \text{x : A. b) : (}\prod \text{ x : A. B)}} \]
 \item Conversion: \[ \frac{\Gamma \text{ }\vdash \text{ A : B } \quad \Gamma \text{ }\vdash \text{ B' : s } \quad \text{B  }=_{\beta} \text{ B' }}{\Gamma \text{ } \vdash \text{ A : B' }} \]
\end{itemize}   



\subsection{Calculus of Construction/Inductive Construction}
\label{sec:cc}
The Calculus of Construction is a higher order  natural deduction style proof system 
for constructive proofs where every proof a typed $\lambda$-abstractions.  Using the 
\textit{Pure Type System} syntax, it can be expressed as:

\begin{displayquote}

    $S$ = \[ \lbrace  Prop \rbrace \cup \lbrace  Type_{i} \mid  i \in \mathbb{N} \rbrace \]
    $A$ =  \[ \lbrace Prop : Type_{0} \rbrace \cup \lbrace Type_{i} : Type_{i+1} \mid i \in \mathbb{N} \rbrace \]
    $R$ = 
     \[
   \left\{ \begin{array}{l}
   (Prop, Type_{i}, Type_{i}) \text{      } i \in \mathbb{N}  \\
   (s, Prop, Prop)  \text{     } s \in S \\
   (Type_{i}, Type_{j}, Type_{max (i, j)}) 
  \end{array}\right\}
\]
       
\end{displayquote}

\noindent
The sort \textit{Prop} captures type of expression which represents logical proposition, while 
the sort \textit{Type} captures the computational content.   Calculus of Construction is
 powerful enough to encode inductive definitions \citep{pfenning1989inductively}, but 
 one of the main drawback is efficiency of computation of function over these encoded 
 inductive definitions, and some other properties could not be proven \citep{10.1007/3-540-45413-6_16}.
In order to solve these problems, \citep{Paulin-Mohring:1993:IDS:645891.671440}  introduced 
Inductive definitions, pattern matching, and fixpoint in the Calculus of Construction to make the data structure 
representation more efficient. Below is the  (incomplete) syntax of Calculus of Inductive Construction:

 \begin{displayquote}

    T = ...  (*  Pure Type System *) \\
       | Ind $\lbrace$ V : T :=  $\textbf{V : T} \rbrace$.V (* inductive definition) \\
       | case T of \textbf{V => T} (* pattern matching *)  \\
       | fix_{n}  $\lbrace$ V : T := T $\rbrace$ (* recursion *) \\
   \end{displayquote}
 

\noindent 
\textit{Inductive Type:} As we mentioned above that inductive types are basic building block for encoding various 
data structures in the Coq (CIC). The keyword to declare a inductive data type in Coq is 
\textit{Inductive}. For example, a length index list
whose elements belong to a type $A$ can defined as (also known as vector):

\begin{minted}{coq}

Inductive Vector (A : Type) : nat -> Type :=
| Nil : Vector A 0
| Cons n : A -> Vector A n -> Vector A (S n).

\end{minted}

\noindent 
Now we can define various functions for the \textit{Vector} data structure. For example, 
we can define a function to append two vectors for length $n$ and $p$ as:

\begin{minted}{coq}
Fixpoint append {A n p} (v : Vector A n) (w : Vector A p)  
  : Vector A (n + p) :=
  match v with
  | Nil _ => w
  | Cons _ _ a v' => Cons _ _ a (append v' w)
  end.

\end{minted}
 
\noindent
The expressiveness of Coq allows to encode various correctness properties at type level. 
In our example of \textit{append}, the correctness criteria states that appending a 
vector of length $n$ with a vector of length $p$ yields a vector of length $(n + p)$.  
In other words, the function \textit{append} is "correct-by-construction". 
During our formalisation, we have encoded our vote counting 
as a \textit{Inductive} data type with various assertions of 
correctness criterion appearing at type level. These assertions at type level 
enforce that only the "correct" term of vote counting inductive data type can be constructed
(\textit{correct-by-construction}).



We would like to point that the current underlying theory of Coq has been 
extended with Co-Inductive types \citep{10.1007/3-540-60579-7_3}; however, 
the discussion of Co-Inductive types is not very relevant for this thesis.	

 
\subsection{Type vs. Prop: Code Extraction}
\label{sec:typeprop}
 Every term in Coq has
 a type, and the term could be either a logical proposition 
 or computational term. 
 The type of logical propositions are \textit{Prop}, while the type of 
 computational parts are \textit{Type}. This distinction between 
 the type of logical propositions (Prop) and the type of computational 
 parts  (Type) provides a mechanism to extract 
 functional programs directly from Coq proof scripts.
 During the extraction process \citep{Letouzey:2008:ECO}, 
 every term of type Prop is removed and no longer exists in extracted 
 code, and only the terms of type Type are translated into target language
 (OCaml/Haskell/Scheme). Because of this, Coq in general does not allow
 the case analysis on
 the terms (logical objects) of sort Prop   when the goal is not in Prop,
 but in certain cases it can be achieved (we call this special 
 case reification and explain next). 
 
 
  
  \subsubsection{Reification}
  \label{sec:reification}
  Sometimes it is very natural to express certain properties/definitions
  in the Prop than the Type. Moreover, the definitions/terms in the Prop are self contained and 
  very intuitive for human understanding. The only problem is that the terms of the type Prop do not carry any 
  computational content but only the proof part. However, 
  we can escape this situation if the term of type Prop is decidable predicate (boolean predicate) and its domain is finite. 
  In the case of decidable predicate in Prop over a finite domain,  we can extract 
  the witness constructively by using 
  a program of linear search that tries the decidable predicate on 
  every element of its (finite) domain. Below is the linear search 
  reification code 
  which produces a Type level witness, \textit{existsT},
  from a Prop level witness, \textit{exists}, by iterating 
  through all the elements of finite type $A$ (the finiteness 
  of $A$ is captured by the list $l$).
  
  
  
\begin{minted}{coq}
Require Import Coq.Lists.List.
Import ListNotations.

(* type level existential quantifier *)
Notation "'existsT' x .. y , p" :=
  (sigT (fun x => .. (sigT (fun y => p)) ..))
    (at level 200, x binder, right associativity,
     format "'[' 'existsT'  '/  ' x  ..  y ,  '/  ' p ']'")
  : type_scope.

(* the following shows that a decidable (or boolean valued) 
   predicate on a finite list
   can always be reified in terms of strong existence *)
Theorem reify {A: Type} (P: A -> bool) : forall (l: list A), 
    (exists x, In x l /\ P x = true) -> existsT x, P x = true.
Proof.
  refine (
      fix Fn l :=
        match l with
        | [] => fun H => _
        | h :: tl => fun H => _
        end).
  contradict H. intro.
  destruct H as [x [H1 H2]].
  firstorder.

  assert (Hbiv: {P h  = true} + {P h <> true}).
  decide equality.
  destruct Hbiv as [Htrue | Hfalse].
  exists h. assumption.
  specialize (Fn tl). apply Fn.
  destruct H as [x [H1 H2]].
  destruct H1. subst.
  firstorder. exists x.
  firstorder.
Defined. 

\end{minted}
    
    
    
  We have used many standard tricks like this  
  to make  our formalization more accessible for human inspection.
  For example, we have two definition, one in Prop and other in Type, 
  of winner, loser and path. The rationale behind two definitions 
  is that Prop definitions are very natural and easy to understand 
  compared to their Type counter part. Furthermore, we have shown that they are 
  equivalent to each other, and used the definitions in Type for
  computation.  Also, there is a nice Coq library 
  ConstructiveEpsilon\footnote{https://coq.github.io/doc/master/stdlib/Coq.Logic.ConstructiveEpsilon.html}
  which uses the similar trick as ours; however, we have not used this library in 
  our formalization. 
 \subsection{Correct by Construction: Type Safe Printf}
 \label{sec:deplambda}
  One of the highly sought feature of Coq is dependent type, 
  a type which is parametrised by value.  
  The expressiveness of dependent type make it possible
  to express specification at type level, and these specifications enables larger 
  set of  logical errors to eliminated at compile time. 
  
 
 Using the expressiveness of dependent type, we construct a type-safe version of 
 printf \citep{Pierce:2004:ATT:1076265}. Our goal is to generate compiler error when the given format string and the type of 
 corresponding input values  
 do not match, e.g. printf "\%d \%s" "hello Coq" 42 should be compiler error because
 \%d is a directive for integer value, but the type of input, "hello Coq", is string. In addition, 
 type-safe printf should print the input when the format string is aligned with type of input, e.g.
 printf "\%s \%d" "hello Coq" 42 should print the string "hello Coq  42" because the first directive 
 of format string, \%s, and type of input, "hello Coq", are aligned. Similarly, the second directive 
 of format string, \%d, is also aligned with the type of input, 42.
 
 The high level idea is to split the printf arguments into two parts: i) format string, 
 and ii) values to be printed. For example, printf "\%s \%d" "hello Coq" 42 would be split into "\%s \%d", and 
 "hello Coq" 42.  Based on the format string, we design two functions: i) a type level function, 
 and ii) a value level function. The type level function would 
 take format string and returns a variadic function type, e.g. 
 on format string "\%s \%d", it would return a function type with 
 signature \texttt{string -> Z  -> string}.
 The value level function, whose type signature 
 is constructed by the type level function, would take the values to printed as input. If the 
 type of values to be printed is aligned with the type constructed by the type level function then 
 we proceed to print the string, otherwise we generate compiler error.  
 


First, we defined a abstract syntax tree, \textit{format}, to make explicit the characters we 
are interested in format string. Additionally, the \textit{format\_string} function takes the format string 
and returns the abstract syntax tree, and the type level, \textit{interp\_format}, takes the 
abstract syntax tree and returns the function type corresponding to format string.

\begin{minted}{coq}
(* abstract syntax tree *)
Inductive format :=
| Fend : format
| Fint : format -> format
| Fstring : format -> format
| Fother : ascii -> format -> format.

(* turn the format string into abstract syntax tree *)
Fixpoint format_string (inp : string) : format :=
  match inp with
  | EmptyString => Fend
  | String ("%"%char) (String ("d"%char) rest) => 
        Fint (format_string rest)
  | String ("%"%char) (String ("s"%char) rest) => 
       Fstring (format_string rest)
  | String c rest => Fother c (format_string rest)
  end.


(* construct the type level function from abstract syntax tree *)
Fixpoint interp_format (f : format) : Type :=
  match f with
  | Fint f => Z  -> interp_format f
  | Fstring f => string -> interp_format f
  | Fother c f => interp_format f
  | Fend => string
  end.
\end{minted}


\noindent
The \textit{interp\_format} function returns a function type 
(\texttt{Z -> string -> string -> string})  on the (abstract syntax tree of) 
format string "\%d \%s \%s" 

\begin{minted}{coq}
Eval compute in interp_format (format_string "%d %s %s").
(* = Z -> string -> string -> string
     : Type *)
\end{minted}

Now, we construct a value level function, \textit{interp\_value}, whose type 
is constructed by type level function, and it will take the values to be printed as input.
The type of values to print should match the type constructed by type level 
function for successful type checking otherwise it will be type error. 

\begin{minted}{coq}
(* value level function whose type is constructed 
    on fly by interp_format function *)
Fixpoint interp_value (f : format) (acc : string) : 
  interp_format f :=
  match f with
  | Fint f' => fun i => interp_value f' (acc ++ of_Z i)
  | Fstring f' => fun i => interp_value f' (acc ++ i)
  | Fother c f' => interp_value f' (acc ++ String c EmptyString)
  | Fend => acc
  end.
\end{minted}

\noindent
Finally, we define the printf function, and evaluate it on two inputs: 
i)  printf "\%d \%s" "hello Coq" 42, and ii)  printf "\%d \%s" 42 "hello Coq".

\begin{minted}{coq}
Definition printf s := interp_value (format_string s) "".           

Eval compute in  printf "\%d \%s" "hello Coq"%string 42.
(* Error: The term ""hello Coq"%string" has type "string" 
while it is expected to have type "Z". *)

Eval compute in  printf "\%d \%s" 42 "hello Coq"%string. 
(*  "\0b101010 \hello Coq"%string. The number 
42 is printed in binary *)                            
\end{minted}
The first input, printf "\%d \%s" "hello Coq" 42, is type error because 
printf "\%d \%s" returns a value level function whose  type is Z -> string -> string, but 
the type of first argument, "hello Coq", is string which does not unifies with Z,
while second one is successfully printed as string. 

  
  

 
 \subsection{Gallina: The Specification Language}
 \label{sec:gallina}
  The example, type safe printf function, I gave in previous 
  section was encoded in Coq's specification language Gallina. 
  Gallina is a highly expressive specification 
  language for development of mathematical theories and proving the    
  theorems about these  theories; however, writing proofs in Gallina
  is very tedious and cumbersome. Furthermore,  it is not suitable for large proof 
  development. In order to ease the proof development, Coq also provides 
  tactics.  The user interacting with Coq theorem prover applies these 
  tactics to build the  Gallina term  which otherwise would  
  be very laborious.
  
 We have written two proofs that addition on natural number is commutative. 
 First proof, \textit{addition\_commutative\_gallina}, is written using 
 Gallina, while the second proof, \textit{addition\_commutative\_tactics}, is written 
 using the tactics.  In general, we write programs directly in Gallina and use tactics 
 to prove properties about the programs. However, there is no fixed set of rules, and tactics 
 can be used to write programs with dependent types (which we have done during this
 formalization).
 
\begin{minted}{coq}
(* proof written using Gallina *)
Lemma addition_commutative_gallina : 
    forall (n m : nat), n + m = m + n.
refine
      (fix Fn (n : nat) : forall m : nat, n + m = m + n :=
        match n as n0 
              return (forall m : nat, n0 + m = m + n0) with
        | 0 => fun m : nat => 
           eq_ind_r (fun n0 : nat => m = n0) 
                     	eq_refl (Nat.add_0_r m)
        | S n' =>
          fun m : nat =>
            eq_ind_r (fun n0 : nat => S n0 = m + S n')
                     (eq_ind_r (fun n0 : nat => S (m + n') = n0) 
                     eq_refl (Nat.add_succ_r m n')) (Fn n' m)
        end).
Qed.

(* proof written using tactics *)
Lemma addition_commutative_tactics :
  forall (n m : nat), n + m = m + n.
  intros n m; try omega.
Qed.
\end{minted}



 \subsection{Trusting Coq proofs}
 \label{sec:coqproof}
  In general, Coq proofs are nowhere similar to a mathematical 
  proof written by trained mathematician. Also, these proofs 
  are verbose and fairly long, so a 
  very fundamental question is: why should we 
  accept or believe in a proof written in Coq \citep{pollack1998believe}?  Generally, the answer of 
	accepting or trusting Coq proofs is two-fold:
  i) is the logic (CIC) sound?, and ii) is the implementation correct?
  The logic has already been reviewed by many peers and proved correct 
  using some meta-logic, therefore the answer of our question about trusting Coq proof 
  hinges on the implementation. 
  The Coq implementation (written in OCaml)  has two parts, the type checker (small kernel), 
  and tactic language to build the proofs.
  We lay our trust in type checker, because it is a small kernel and can be 
  manually inspected. Furthermore, if there
  is a bug in tactic language, which often is the case, then build proof would 
  not pass the type checker.  Also, we can use the publicly available proof 
  checkers written by experts and inspected by many others. In addition, to increase the 
  confidence, there have been 
  efforts to certify type checker \citep{Appel2003}
  \citep{barras1996coq}, verifying meta theory of one proof system 
  in other \citep{10.1007/978-3-319-08970-6_3}, self certificate of 
  theorem prover \citep{10.1007/11814771_17}. However, no system can 
  prove its own consistency (G{\"o}del's second incompleteness theorem), therefore
  trusting human judgement is inevitable.
  
 
\section{Cryptography}
\label{sec:cryptography}
    The word cryptography comes from the two Greek words: 
    \textit{krypt\'{o}s}, meaning \textit{hidden}, and \textit{gr\'{a}fein} meaning \textit{to write}. As a matter of 
    fact, in the past, hidden writing (cryptography), using the symbol replacement, has been used 
    to conceal the message. For example,
    the earliest known usage of cryptography (symbol replacement) goes back to  ancient 
    Egyptian (Khnumhotep {\rm II}, 1500 BCE); however, the purpose of replacing one symbol by other 
    was not to protect
    any sensitive information but to enhance the linguistic appeal. The first known usage of 
    cryptography to conceal the sensitive information goes back Mesopotamians (1500 BCE) where 
    they used it to hide the formula for pottery glaze. Fast forward, around 100 BCE, 
    Julius Caesar wrote a letter to Marcus  Cicero using a method, now known 
    as \textit{Caesar cipher}, which would shift each character in letter by 3 position right with wrapping 
    around, i.e. X would wrap A, Y would wrap to B, and Z would wrap to C. Decryption was 
	3 character left shift.  Using the  tools of modern mathematics, encryption and decryption 
	in \textit{Caesar cipher} is modular addition and modular subtraction (modulo 26), respectively.  
    Overall, cryptography is art and science of making thing  unintelligible from everyone, except the 
    intended recipient.  	
	    
	The modern day cryptography originated in 1970 with two ingenious ideas, \textit{Data Encryption Standard (DES)}, 
	and \textit{Diffie-Hellman Algorithm}. Data Encryption Standard, developed at IBM in 1970, is a symmetric 
	key encryption algorithm which uses the same key for encryption and decryption. Since its inception, Data Encryption Standard
	amassed a bad reputation because of \textit{National Security Agency (NSA)} involvement; however, it had a 
	practicality issue: key management. If the two parties wanted to communicate
	securely over insecure channel using  the Data Encryption Standard, then they needed to agree on a common key. 
	In order to agree on the common key, they needed a secure channel where they can securely communicate the key. 
    The solution to this problem came from \textit{Diffie-Hellman} key exchange where two parties can exchange the 
    key securely over insecure channel. Moreover, the advent of \textit{Diffie-Hellman} key exchange started the 
    whole new area of public key cryptography where encryption and decryption key are different. 
    Although \textit{Diffie-Hellman} key exchange suffers from  man-in-the-middle (MITM)  attack if used for keys exchange in its 
    naivety, e.g. Logjam \citep{Adrian:2015:IFS:2810103.2813707}. Nonetheless, 
    it is a building block ElGamal Encryption \citep{elgamal1985public}. 
    
    
    
    In this thesis, we are mostly concern about public key cryptography.
    The basic working principles of modern day cryptography is based on 
    mathematical principle than vanilla symbol replacement. 
    In addition, it is no longer just 
    used to achieving confidentiality, but various other things, e.g. integrity,  authentication, non-repudiation 
    protocol, digital signature, digital cash, etc.
    These mathematical principle involves 
    various algebraic structures and algorithm to manipulate the object from these structures.
    For example, the underlying mathematical principal of \textit{Diffie-Hellman}  algorithm 
    is hardness of computing discrete logarithms in finite fields.
    
    
  
    
    Now we  describe the workings of Diffie-Hellman \citep{Diffie:2006:NDC:2263321.2269104}
    algorithm, because all the constructions we have used  are based on Diffie-Hellman construction. 
    Before we describe the algorithm, we briefly sketch the algebraic structure Group because it is underlying algebraic structure of 
    Diffie-Hellman  construction  (typically, the underlying 
    structure is multiplicative group of a finite field). Also, note that our definition is influenced theorem-provers/type-theory because 
    we have  written the type signature of group operator $*$ and inverse operator $inv$. 
    
    \subsection{Group}
    \label{sec:group}
    A group is a set $G$, with a binary operator $* : G \rightarrow G \rightarrow G$, identity element $e$, and inverse operator $inv : G \rightarrow G$ such 
    that the following laws hold: 
    \begin{itemize}
     \item \texttt{Associativity}: $\forall$  a b c $\in G,$  $a * (b * c) = (a * b) * c$
    \item \texttt{Closure}: $\forall$ a b $\in G,$  $a * b \in G$
    \item \texttt{Inverse Element}: $\forall$ a $\in G$ $\exists$ $a^{-1} \in G$, such that $a * a^{-1} = a^{-1} * a = e$. $a^{-1}$ is called inverse of a (
     $inv$ $a$).
    \item \texttt{Identity}: $\forall$ a $\in G,$  $a * e = e * a  = a$
    \end{itemize}
   
    \noindent
    Furthermore, if a group is commutative, i.e. 
    $\forall$ a b $\in  G,$  $a * b = b * a$, then we call it abelian group (in honour of Niels Henrik Abel). In addition, 
    if a \textit{group} is \textit{cyclic group} if it can be generated by a single element, also known as generator of group 
    and denoted as $g$, by repeatedly applying the group operator $*$ to itself. Moreover, a group is \textit{finite cyclic group}
    if it is cyclic and the cardinality of the underlying set (carrier set) $G$ is finite. The cardinality is also known as order of group. 
	    
    
     
     \subsection{Diffie-Hellman Construction}
     \label{sec:diffie-hellman}
     	Now we explain \texttt{Diffie-Hellman} construction. The construction can be divided into two steps:
		\begin{enumerate}
		\item The two communicating parties, say Alice and Bob, agree with shared public parameters which 
		are finite cyclic group $G$ of order $p$ ($p$ is a large prime) and generator element $g$.
		\item After agreeing with public parameters, Alice and Bob initiates the key exchange protocol (assuming that 
		 Alice goes first):
		 \begin{enumerate}
		   \item Alice selects a random number $a$, where 1 < $a$ < $p$, computes $g^{a}$ ( $g * g * g ... * g$  $a$ times), and shares 
		   $g^{a}$ with Bob. 
		   \item Similarly, Bob selects are random number $b$, where 1 < $b$ < $p$, computes $g^{b}$, and shares  $g^{b}$
		   with Alice.
		   \item Finally, Alice computes the key $(g^{b})^{a}$, and Bob computes the key $(g^{a})^{b}$.  A basic 
		   algebraic simplification on Alice's key and Bob's key would show that they both have the 
		   common key  $g^{ab}$.
		   
		 \end{enumerate}
      \end{enumerate}		
      
     
      \noindent
       During the whole process, Eve, the adversary, would have $g^{a}$ and $g^{b}$, but she can not compute the 
      $ g^{ab}$ from these two values assuming that discrete logarithm is hard to compute. 
      There are, off course, other attacks exists, e.g. denial of man in the middle attack, Logjam, etc. 
      The security property of \texttt{Diffie-Hellman} construction is formalized using complexity theoretic notion 
      given below (we would not go into the details of complexity theoretic notions):
      
     
      \textbf{DL - Discrete Logarithm problem:} 
      An instance of \textit{DL} problem states that given a finite cyclic group $G$, a generator of $g$ of $G$, and 
      an element $y$, finding an element $x \in G$ such that $g^{x} = y$.
      
      
      \textbf{DH - Diffie-Hellman problem:}
      An instance of \textit{DH} problem   
      states that given a finite cyclic group $G$, a generator of $g$ of $G$, 
      elements $g^{a}$ and $g^{b}$, finding the element $g^{ab}$.
      
      
      \textbf{DDH - Decision Diffie-Hellman Problem:}
      An instance of \textit{DDH} problem states that given a finite cyclic group $G$, a generator of $g$ of $G$,
      elements $g^{a}$, $g^{b}$, and $g^{c}$, determining if $c = a * b$.
      
           	 
     
     \subsection{ElGamal Encryption Scheme}
     \label{sec:elgamal}
     In 1985, Tahir ElGamal \citep{elgamal1985public} proposed a new encryption system which was based on Diffie-Hellman algorithm. 
     \textit{ElGamal} turned the interactive  Diffie-Hellman algorithm into a non-interactive, no need for any active second party, by introducing 
     a randomness.  The \textit{ElGamal} scheme has three phases:
     \begin{enumerate}
		\item \textbf{Key Generation:}   
		The user, say Alice, first chooses a finite-cyclic group $G$ of order $p$ ($p$ is a large prime) and a group group generator $g$.
		She randomly selects a an element $x$ from $\{1, \ldots, p-1\}$ as a private key, computes her public key $h = g^{x}$. 
		Subsequently, she publishes 
		the <$G$, $g$, $p$, $h$> and keeps $x$ private. 
		\item \textbf{Encryption:}
		If any party, say Bob, wants to send a encrypted message $m$ to Alice, then he would randomly select an element 
		r, where 1 < r < p, computes $c_{1} := g^{r}$ and $c_{2} := m * h^{r}$, and send the pair ($c_{1}, c_{2}$) to 
		Alice. 
		\item \textbf{Decryption:}
		Upon receiving any pair ($c_{1}, c_{2}$), Alice would compute $c_{2} * c_{1}^{-x}$. A basic simplification of $c_{2} * c_{1}^{-x}$
		shows that it recovers the plaintext message. The simplification proceeds by replacing the $c_{2}$ with 
		$m*h^{r}$ and $c_{1}$ with $g^{r}$ in $c_{2} * c_{1}^{-x}$. This substitution leads to 
		$m * h ^ {r} * g^{-rx}$ which upon further simplification by replacing the $h$ with $g^{x}$
		leads to $m * g^{xr} * g^{-rx}$. Using the same base rule, the term $m * g^{xr} * g^{-rx}$ can be 
		written as $m * g^{xr - rx}$. Since $x  r = r x$, so we 
		can replace $m * g^{xr - rx}$ with $m * g^{0}$. The term $g^{0}$  = $e$ (the identity of group $G$) and using 
		the right identity group law, we can replace $m * e$ by $m$. 
		
	\end{enumerate}	
    
    \subsection{Homomorphic Encryption}
    \label{sec:homomorphic-enc}
	 Homomorphic encryption  is a encryption scheme which allows us to perform useful operation on 
	     encrypted data without decrypting the data.
	     It was first posed by Rivest, Adleman and Dertouzos in \citep{rivest1978data}: 
      
       \begin{displayquote}  
	     Consider a small loan company which uses a commercial time-sharing service to store its records.  
	     The loan company’s "data bank" obviously contains sensitive information which should be kept private.  
	     On the other hand, suppose that the information protection techniques employed by the time sharing 
	     service are not considered adequate by the loan company.  In particular, the systems programmers would 
	     presumably have access to the sensitive information.  The loan company therefore decides to encrypt all 
	     of its data kept in the data bank and to maintain a policy of only decrypting data at the home office -- data 
	     will never be decrypted by the time-shared computer.
	   \end{displayquote}  

 \noindent	     
 A encryption scheme is homomorphic if for any two plaintext $x$ and $y$:
		\begin{displayquote}
		
		 $Enc_{pk}(x) \bigotimes Enc_{pk}(y) = Enc_{pk} (x \bigoplus y)$  where 
		$Enc$ is encryption function, $pk$ is the public key, $\bigotimes$ is operation on ciphertext, and $\bigoplus$
		 is operation on plaintext.
		
		\end{displayquote}
				
		These two operators $\bigotimes$ and $\bigoplus$ are very specific. If a cryptosystem that supports an arbitrary 
		function $f$ on ciphertext, then it is called fully homomorphic cryptosystem:
		\begin{displayquote}
		  $ f (Enc_{pk}(m_{1}), Enc_{pk}(m_{2}), ..., Enc_[pk}(m_{k}) = Enc_{pk}( f (m_{1}, m_{2}, ..., m_{k}))$ 
	    \end{displayquote}
		
		\noindent
		The first fully homomorphic encryption system was proposed by Craig Gentry \citep{Gentry:2009:FHE:1834954}; however, 
		in this thesis we are mostly concern with partially homomorphic encryption (either additive or multiplicative, but not both),
		specifically additive ElGamal, 
		so we are not going to present the details overview 
		of Craig Gentry fully homomorphic construction. From now on, we would be using the term homomorphic encryption for 
		partially homomorphic encryption. 
		 	    
	    
	    
	    
	     
	    Now, keeping in mind that homomorphic encryption enables us to perform useful operation on encrypted data, 
	    we will see what kind of homomorphic property is exhibited by the ElGamal method discussed in the previous section. 
	    Given a public infrastructure <$G, p, g, h$> for ElGamal scheme, 
	     we encrypt two message $m_{1}$ and $m_{2}$ by taking two random numbers $r_{1}$,  $r_{2}$ from the group:
	     \begin{displayquote}
	     $Enc(m_{1}, r_{1}) := (g^{r_{1}}, m_{1} *  h^{r_{1}})$ 
	      \end{displayquote}
	     
	     \begin{displayquote}
	     $Enc(m_{2}, r_{2}) := (g^{r_{2}}, m_{2} *  h^{r_{2}})$ 
	      \end{displayquote}
	     
	     
	     If we mutiply these two cipher together pairwise, we get ($g^{r_{1}+ r_{2}}, m_{1} * m_{2} *  h^{r_{1} + r_{2}}$). 
	     After decrypting this combined ciphertext, we will get $m_{1} * m_{2}$. In the scheme, $\bigotimes$ is multiplication and 
	     $\bigoplus$ is also multiplication. Furthermore,  
	     if our end goal is  to achieve multiplication on a bunch of plaintext, then rather than decrypting the corresponding ciphertext individually 
	     and multiplying them, we could simply multiply all the ciphertext together and decrypt the final result.  
	     The advantage of this scheme is that it does not leak the individual values which, sometimes, is a very crucial property in many 
	     application, specifically election voting.
	     In electronic voting protocols, we do not want to reveal the choices of a individual voter, but it is acceptable to reveal the final tally. 
	     However, this scheme is not suitable for electronic voting schemes because it is multiplicative. Almost, to the best of 
	     my knowledge, all the electronic voting scheme calculate the finally tally by adding the individual choices of all
	     voters, so the requirement is achieve the addition on plaintext. 
	     There are 
	     many additive homomorphic encryption schemes, e.g. Benaloh cryptosystem, Paillier cryptosystem, etc. In addition, we 
	     can modify the ElGamal encryption scheme to make additive. In additive case, it works as:
	       \begin{displayquote}
	      $Enc(m_{1}, r_{1}) := (g^{r_{1}}, g^{m_{1}} *  h^{r_{1}})$ 
	      \end{displayquote}
	     
	     \begin{displayquote}
	    $Enc(m_{2}, r_{2}) := (g^{r_{2}}, g^{m_{2}} *  h^{r_{2}})$ 
	      \end{displayquote} 
	     
	    
	     
	    
	      \noindent
	      Multiplying these two ciphers pairwise would give us,  ($g^{r_{1} + r_{2}}$, $g^{m_{1} + m_{2}} * h^{r_{1} + r_{2}}$) which would decrypt as 
	      $g^{m_{1} + m_{2}}$. In this case, $\bigtimes$ is multiplication and $\bigoplus$ is addition.  We can calculate the value 
	      of $m_{1} + m_{2}$ by using linear search algorithm, or more efficient one 
	      Pohlig–Hellman algorithm \citep{10.1109/TIT.1978.1055817}. 
	      However, the downside 
	      of this scheme is that if the values of $m_{1} + m_{2} + \dotsb + m_{n}$ (assuming n values) is very large, then calculating it from 
	      $g^{m_{1} + m_{2} + \dotsb  + m_{n}}$ is 
	      not very practical \citep{10.1007/3-540-69053-0_9}. 
	     
     \subsection{Zero-Knowledge Proof}
     \label{sec:zkp}
      In conventional mathematics, a proof of mathematical statement is collection of basic axioms combined according to rules of 
      the system. For example, we want to prove that in for any group G with group operation *, for any two elements x y $\in$ G, we have:
      \begin{displayquote}
		 
		 $(x * y)^{-1}$ = $y^{-1} * x^{-1}$
		
	    \end{displayquote}
      
      \noindent
      Proof: we assume arbitrary x, y. We show that $(x * y)^{-1}$ and $y^{-1} * x^{-1}$ are 
      inverse of each other by combining them together using the group operator $*$ and using 
      the group laws lead to the identity of the group $G$.
 
 \begin{align}
(x * y) * (y^{-1} * x^{-1})&=  x * y * y^{-1} * x^{-1}  (associativity) \nonumber \\
                     &= x * (y * y^{-1}) * x^{-1}   (associativity) \nonumber \\
                     &= x * e  * x^{-1} (inverses) \nonumber \\
                     &=  x  * x^{-1} (identity)   \nonumber \\
                     &= e (inverse) \nonumber \\
\end{align}

     \noindent
      Similarly, we can prove that $((y^{-1} * x^{-1}) *   (x * y) = e $.
      We can also formalize it inside theorem prover and prove it more formally (below is a proof in Coq theorem prover where 
      $*$, the group operation, is represented as $f$ and $^{-1}$, the inverse operation, is represented as $inv$).
      
      \begin{minted}{coq}
      Lemma inv_distr : forall a b, inv (f a b) = f (inv b) (inv a).
      Proof.
        intros a b. symmetry. 
        apply inv_uniq_l.
        rewrite <- assoc.
        rewrite  (assoc (inv b) (inv a) a).
        rewrite (inv_l a).
        rewrite (assoc (inv b) e b).
        rewrite (id_l b).
        rewrite (inv_l b). auto.
      Qed.
      \end{minted}
     
     If a verifier wants to verify the correctness of our proof, then she would simply check that if the group rules are applied correctly. 
     Moreover, these proofs 
     are static in nature, i.e. once the prover has produced the proof, then the content of proof is not going to change over time, and
     there would not be any interaction between prover and verifier if verifier wants to verify the proof.  In addition, the verifier 
     not only learned that the statement is true, but she also learned the content of proof (gained some knowledge).
     
     In contrast, zero-knowledge-proof, first introduced by Goldwasser, Micali, and Rackoff \citep{Goldwasser:1985:STOC} , 
     is probabilistic proof that involves the explicit notion of a interaction between 
     the prover and verifier. In addition, 
     the goal of the prover is to convince the verifier about the validity of some statement without revealing any information, i.e. 
     the only thing verifier would learn is that if statement is true or false without any other information. 
     More formally, zero-knowledge proof for a language $L \in \{0, 1\}^{*} $ (generally NP) is a interactive proof  
     between a (computationally unbounded) prover $P$ and a (polynomial time) verifier $V$. Furthermore, 
     the goal of $P$ is to convince V that x $\in$ L  such that:
   
     \textbf{Completeness:} If x $\in$ L then the honest prover $P$ would convince the 
       honest verifier V to accept the claim with overwhelming probability. 
       If $P$ can always convince (probability 1) the $V$ that x $\in$ L, then the proof system has perfect completeness. 
    
     \textbf{Soundness:} If x $\notin$ L then dishonest prover $P^{*}$ can not convince the honest verifier $V$ 
     to accept the claim (with some small probability error known as soundness error)
     
     \textbf{zero-knowledge:} A malicious verifier $V^{*}$ would gain no additional information by interacting with a honest prover $P$ 
      other than x $\in$ L. More formally, for every (polynomial time) program $V^{*}$ there exists a (polynomial time)
      program $S$, also known as simulator, which can produce the transcript of protocol by itself without interactive with anyone. 
      Moreover, the transcript 
      produced by simulator $S$ is indistinguishable from real transcript produced by interaction between 
      
    
    \subsubsection{Zero-Knowledge Proof of Knowledge}
    Sometimes, the fact that $ x \in L$ is completely trivial.  For example, for any given finite group $G$ of order $p$ ($p$ is prime), 
    a random element $h$ from the group $G$, and generator $g$of the group $G$, a prover claims that there is a $x$ such that 
    $g^x = h$. 
    This is trivial because we know that there always exists such $x$ (discrete logarithm problem); however, the challenge is to show that
    the prover knows the witness $x$.
    Formally, zero-knowledge proof of knowledge is defines as: let $R = {(x, w) \subset L \times W$ is a binary relation such that     
    $x \in L$ is common string between prover $P$ and verifier $V$ and $w \in W$, also known as witness, is private to 
    the prover $P$.  Moreover, the goal of prover $P$ is to convince verifier $V$ that $(x, w) \in R$ in zero-knowledge.  
    
        
   
    \subsection{Sigma Protocol}
    \label{sec:sigma}
     Sigma protocols are efficient way to achieve zero-knowledge proof of knowledge.   Sigma protocol is 
     a three step communication between a prover $P$ and a verifier $V$ where goal of the prover is to convince the verifier that 
     she knows witness $w$ for common input $x$ such that  $(x, w) \in R$: 
     
     \begin{enumerate}
     \item $P$ sends a message $a$
     \item $V$ sends a random string $e$
     \item $P$ replies with $z$
     \end{enumerate}
     
     Based on public inputs $(x, a, e, z)$, the verifier $V$ decides to accept or reject the proof.   A protocol is 
     said to be sigma protocol for a relation $R$ if: 
     
     \textbf{Completeness:} when prover and verifier follow the protocol for public input $x$ and witness $w$ 
          then verifier accepts the proof
          
      \textbf{Special Soundness:} For a given pubic input $x$, if prover can produce two accepting transcript $(a, e, z)$ 
      and $(a, e', z')$ ($e$ and $e'$ are disjoint), then there exists a efficient program, extractor, which can extract the 
      witness w.
      
      \textbf{Honest Verifier Zero-Knowledge:} For a given public input $x$ and random input $e$, there is a simulator 
      which outputs an accepting transcript $(a, e, z)$ which is indistinguishable from a proof generated by 
      a prover interacting with honest verifier. 
     
     A concrete example of sigma protocol is Schnorr protocol \citep{10.1007/3-540-48658-5_19}. In this example, 
     the goal of a prover $P$ is
     to prove the knowledge of discrete log in a Group of order $p$ (p is prime) to a verifier $V$.
     Furthermore, $g$ is the generator of 
     group $G$, $x$ is the public input and $w$ is private input with relation $x = g^w$. The protocol follows:
     
     \begin{itemize}
     \item Prover $P$ randomly selects an element $r$ from [0 $\dotsb$ q), computes $a = g^r$ and sends $a$ to verifier $V$
     \item Verifier $V$ randomly selects an element $c$ from [0 $\dotsb$ q) and sends it to $P$
     \item Prover $P$ sends $z = r + c * w $ to $V$.  $V$ checks $g^{z} = a * x^{c}$
     \end{itemize}
     
     For the protocol described above, all three properties, completeness, special soundness, and honest 
     verifier zero-knowledge, hold. 
     \begin{itemize}
      \item Completeness holds with probability $1$. Simplifying the expression $g^{z}$ shows that 
      it is equal to $a * x^{c}$. Replacing the $z$ by $r + c * w$ in expression  $g^{z}$, we get 
      $g^{r + c * w}$.  Using addition rule of power, $g^{r + c * w}$ can be simplified as 
      $g^{r} * g^{c * w}$. First first step of protocol, $a = g^r$, so we can replace the $g^{r} * g^{c * w}$ 
      by $a * g^{c * w}$. From the group infrastructure, we have $x = g^w$, so we can write $x$ at place of 
      $g^{w}$, therefore, $a * g^{c * w}$ transforms into $a * x^c$. 
      
     \item Special soundness holds. For any two given response, 
     $z_{1} = r + w * c_{1}$ and  $z_{2} = r + w * c_{2}$, we can find the witness w by  $(z_{2} - z_{1})/(c_{2} - c_{1})$.
     
     \item Honest verifier zero-knowledge also holds. Simulator can always produce a transcript $(g^{z} x^{-c}, c, z)$ by randomly 
     choosing $c$ (the random choice $c$ is the reason for special honest verifier zero-knowledge), and $z$. 
     \end{itemize}
     
     \textbf{Fiat-Shamir Transform:}
      In practice, the \textit{Fiat-Shamir} transform is used to turn a Sigma protocol into a non-interactive proof. 
      As a consequence, there is no longer any interaction with verifier. A \textit{Fiat-Shamir}  transform to sigma protocol is:
      
      \begin{itemize}
     \item Prover $P$ randomly selects an element $r$ from [0 $\dotsb$ q), computes $a = g^r$
     \item Prover $P$ computes $c =  H(a || x) $ where $H$ is a hash function
     \item Prover $P$ computer $z := r + c  * w $
     \end{itemize}
      Finally, $P$ publishes the transcript $(a, c, z)$ for anyone to verify her claim. Subsequently, 
      any one who is verifying the claim has to check two things: (i) $c := H (a || x)$, and (ii) 
      $g^{z} = a * x^{c}$. 
      
   
     \subsection{Commitment Schemes}   
     \label{sec:commscheme}
      Commitment schemes are cryptographic primitives equivalent to real life sealed lock-box.
      Once the lock-box is locked and sealed, the content inside it can not be changed without breaking the lock and seal. 
      In general, commitment primitives 
      are backbone of any cryptographic protocol between two parties, communicating over internet, to force them  to  follow the 
      protocol honestly, even they would have a huge gain from deviating 
      from protocol. For example, in order to save some time before a match, Indian cricket team captain, living in Delhi, and Australian cricket 
      team captain, living in Canberra, decide 
      to toss a coin in advance over the Internet, using a mobile application  called toss-app, for a  upcoming series of one-day matches
      \footnote{
      In a cricket match, which is very popular sport in India and Australia, both captains meet in the ground and toss a coin to 
      decide who would have the first call.}.  Assuming the workings of toss-app is naive, i.e. one captain is going to post
      his call in the chat box, and the other other captain is going to toss the coin at their end and post the outcome in chat box. 
      Furthermore, the decision is taken based on the messages posted by the two captains. In this scenario, we are assuming that 
      the captain, who is tossing the coin, is honest and posting the outcome of the coin honestly in the chat box, which could or 
      could not be the case.  The question is can we devise some scheme which would force the both parties to behave honestly? 
      The answer is yes, we can devise such scheme. We would use the sealed lock-box concept, albeit the digital one. Moreover, 
      the first captain would put his call in digitally sealed lock-box and post it in the chat box. Because it is sealed and 
      locked, the other captain would have no idea what is the content inside it. Furthermore, it is impossible to break 
      the lock box, so it is fruitless and waste of time for the other captain to even try. The other captain will toss the coin 
      and post the outcome in a separate digital sealed lock box. Now that we two digital sealed lock box which can 
      only be opened by the respective owners, they would move for the next phase of coin tossing  called revealed phase. 
      In the revealed phase, they both would open their sealed locked box to show that what they have locked, 
      and the decision would be taken accordingly \footnote{Story influenced by Manuel Blum's coin flipping by telephone}. 
      
      
      Formally, a commitment scheme is three step protocol between a sender $S$ and a receiver $R$:
      \begin{enumerate}
      \item Commit phase: sender $S$ commits a value $m$ by generating a random number $r$ and using some algorithm $C$, which takes the message 
      and random $r$. Moreover, the committed value produced by the commitment algorithm $C$, $c = C(m, r)$, is shared with receiver $R$.
      \item Reveal phase: In the reveal phase, the sender reveals the message $m$ and randomness $r$ which are subsequently used by 
         receiver to verify the result, i.e. the receiver computes  $c' = C(m, r)$ and matches it again the given $c$ in the commit phase of protocol. 
      \end{enumerate}
      
      
    \textbf{Security Properties:} 
     Commitment schemes have to have two properties: hiding and binding. Hiding property ensures that 
     the receiver can not recover or recompute the original message $m$ from the given commitment $c$, i.e. 
     it forces the receiver to behave honestly in the protocol. 
     Furthermore, binding property ensures that it is impossible for sender to come up with 
     another message $m'$ which is different from $m$ but produces the same commitment $c$, i.e. 
     it forces the sender to behave honestly in the protocol. 
     
    \textbf{Pedersen commitment:}
     Finally, we give a brief overview of a Pedersen commitment scheme which is based on discrete log.  The protocol as follows assuming 
     the public parameter available to sender and receiver, i.e. the set up has been conducted to generate the the public parameter, 
     and both parties have these values. These values include a prime $p$, $y$ a randomly chosen element from $Z_{p}^{*}$, and $g$ 
     a randomly chosen generator from   $Z_{p}^{*}$.  
     
     \begin{itemize}
     \item Commit phase: The sender generates a random $r$ from $Z_{p}^{*}$, computes commitment $c = g^{r}*y^{m}$ and sent the commitment to 
        receiver
      \item Verification phase: In verification phase, the sender reveals the original message $m$ and the randomness $r$. Finally, 
            the receiver computes  $g^{r}*y^{m}$. If the computed  value matches with the commitment received in 
            commitment phase, then she accepts it otherwise reject it. 
     
	\end{itemize}      

	

\section{Summary}
In this chapter, we gave a brief summary of Coq theorem prover and cryptographic notation needed to understand the further chapters. By no means, 
these descriptions were exhaustive. For a detailed treatment of Coq theorem prover,   \citep{Bertot:2004:ITP} \citep{Chlipala:2013:CPD:2584504}
 can be referred, and for cryptography,    \citep{Menezes:1996:HAC:548089} \citep{Schneier:1995:ACP:212584} \citep{Paar:2009:UCT:1721909} 
can be referred.  In the next chapter, we will discuss the Schulze method, and the machinery for its formalization. 




