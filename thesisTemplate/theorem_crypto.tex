\chapter{Theorem Prover and Cryptography}
\label{cha:theorem_crypto}
 

A proof assistant or theorem prover is a computer program which assists users in development 
of mathematical proofs. Basically, the idea of 
developing mathematical proofs using computer goes back to Automath (automating mathematics)
\citep{deBruijn1983} and LCF \citep{Milner:1972:IAS:942578.807067}. The 
Automath project (1967 until the early 80's)  was initiative of De Bruijn, and the aim of the project was to 
develop a language for expressing mathematical theories which can be verified by aid of computer.  
Moreover, the Automath was first 
practical project to exploit the Curry-Howard isomorphism (proofs-as-programs and formulas-as-types). 
DeBruijn  was likely unaware of this correspondence, and he almost re-invented it.
The Automath project can be seen as the precursor of
 proof assistants NuPrl \citep{Constable:1986:IMN:10510} and Coq \citep{Bertot:2004:ITP}.  
 Some other notable  proof assistants are 
 Nqthm/ACl2 \citep{507872}, PVS \citep{Owre:1992:PPV:648230.752639},
 HOL (a family of tools derived from LCF theorem prover) \citep{Slind:2008:BOH:1459784.1459792}
 \citep{Harrison:1996:HLT:646184.682934} \citep{Nipkow:2002:IHP},
 Agda \citep{Norell:2008:DTP:1813347.1813352}, and Lean \citep{10.1007/978-3-319-21401-6_26}.


\textbf{Chapter overview:}
 This chapter is overview of Coq theorem prover and Cryptographic primitives. 
 In the section \ref{sec:problemstatement}, I will give a brief overview of 
 theoretical foundation, calculus of construction (\ref{sec:cc}) and calculus of inductive 
 construction (\ref{sec:cic}), of Coq.  In the section \ref{sec:typeprop}, I will discuss about 
 \texttt{Type} and \texttt{Prop}
 which is very crucial from program extraction point of view.  Our goal during 
 this course of formalization was not only proving the correctness of 
 Schulze method, but extracting a executable OCaml/Haskell code to count 
 ballots.  In the section \ref{sec:deplambda}, I will focus 
 on dependent types and  how it leads to correct by construction paradigm
 by designing a  type safe printf function. 
 Section \ref{sec:gallina} focuses on Coq specification language 
 \texttt{Gallina} with a example showing that why writing proofs using  
 \texttt{Gallina} is difficult and cumbersome, and how it can be eased by 
 using tactics. Finally, in the section  \ref{sec:coqproof}, we will take 
 philosophical route to justify that why should we trust in Coq proofs 
 even though they do not appear anywhere near to a proof written by 
 a human. 


\section{Coq: Interactive Proof Assistant}
\label{sec:problemstatement}
Coq  is an interactive proof assistant (theorem prover) based on
theory of Calculus of 
Inductive Construction \citep{Paulin-Mohring:1993:IDS:645891.671440} which itself is an 
augmentation of Calculus of Construction 
\citep{Coquand:1988:CC:47724.47725} with inductive data-type.  
 

\subsection{Calculus of Construction}
\label{sec:cc}
\subsection{Calculus of Inductive Construction}
\label{sec:cic}
%\fix{Flesh out the details of 
% Calculus of construction and Inductive construction}
% \fix{Write here about syntax and semantics of CIC}
 
 
\subsection{Type vs. Prop: Code Extraction}
\label{sec:typeprop}
 \fix{explain here the difference between Prop and Types. How it affects 
  the code extraction }
  It's good starting point to tell the reader that we have two definitions, 
  one in type and other in prop. Why ? Because Type computes, but it's
  not very intuitive for human inspection while Prop does not compute, 
  but it's very intuitive for human inspection. We have connected that 
  the definition expressed in Type is equivalent to Prop definition. 
  
  \subsubsection{Reification}
  \label{sec:reification}
    
 \subsection{Correct by Construction: Type Safe Printf}
 \label{sec:deplambda}
  One of the highly sought feature of Coq is dependent type, 
  a type which is parametrised by value.  
  The expressiveness of dependent type make it possible
  to express specification at type level, and these specifications enables larger 
  set of  logical errors to eliminated at compile time. 
  
 
 Using the expressiveness of dependent type, we construct a type-safe version of 
 printf. Our goal is to generate compiler error when the given format string and the type of 
 corresponding input values  
 do not match, e.g. printf "\%d \%s" "hello Coq" 42 should be compiler error because
 \%d is a directive for integer value, but the type of input, "hello Coq", is string. In addition, 
 type-safe printf should print the input when the format string is aligned with type of input, e.g.
 printf "\%s \%d" "hello Coq" 42 should print the string "hello Coq  42" because the first directive 
 of format string, \%s, and type of input, "hello Coq", is aligned. Similarly, the second directive 
 of format string, \%d, is also aligned with the type of input, 42.
 
 The high level idea is to split the printf arguments into two parts: i) format string, 
 and ii) values to be printed. For example, printf "\%s \%d" "hello Coq" 42 would be split into "\%s \%d", and 
 "hello Coq" 42.  Based on the format string, we design two functions: i) a type level function, 
 and ii) a value level function. The type level function would 
 take format string and returns a variadic function type, e.g. 
 on format string "\%s \%d", it would return a function type with 
 signature \texttt{string -> Z  -> string}.
 The value level function, whose type signature 
 is constructed by the type level function, would take the values to printed as input. If the 
 type of values to be printed is aligned with the type constructed by the type level function then 
 we proceed to print the string, otherwise we generate compiler error.  
 


First, we defined a abstract syntax tree, \textit{format}, to make it explicit the characters we 
are interested in format string. Additionally, the \textit{format\_string} function takes the format string 
and returns the abstract syntax tree, and the type level, \textit{interp\_format}, takes the 
abstract syntax tree and returns the function type corresponding to format string.

\begin{verbatim}
(* abstract syntax tree *)
Inductive format :=
| Fend : format
| Fint : format -> format
| Fstring : format -> format
| Fother : ascii -> format -> format.

(* turn the format string into abstract syntax tree *)
Fixpoint format_string (inp : string) : format :=
  match inp with
  | EmptyString => Fend
  | String ("%"%char) (String ("d"%char) rest) => Fint (format_string rest)
  | String ("%"%char) (String ("s"%char) rest) => Fstring (format_string rest)
  | String c rest => Fother c (format_string rest)
  end.


(* construct the type level function from abstract syntax tree *)
Fixpoint interp_format (f : format) : Type :=
  match f with
  | Fint f => Z  -> interp_format f
  | Fstring f => string -> interp_format f
  | Fother c f => interp_format f
  | Fend => string
  end.
\end{verbatim}


\noindent
The \textit{interp\_format} function returns a function type 
(\texttt{Z -> string -> string -> string})  on the (abstract syntax tree of) 
format string "\%d \%s \%s" 

\begin{verbatim}
Eval compute in interp_format (format_string "%d %s %s").
(* = Z -> string -> string -> string
     : Type *)
\end{verbatim}

Now, we construct a value level function, \textit{interp\_value}, whose type 
is constructed by type level function, and it will take the values to be printed as input.
The type of values to print should match the type constructed by type level 
function for successful type checking otherwise it will be type error. 

\begin{verbatim}
(* value level function whose type is constructed on fly by interp_format 
    function *)
Fixpoint interp_value (f : format) (acc : string) : interp_format f :=
  match f with
  | Fint f' => fun i => interp_value f' (acc ++ of_Z i)
  | Fstring f' => fun i => interp_value f' (acc ++ i)
  | Fother c f' => interp_value f' (acc ++ String c EmptyString)
  | Fend => acc
  end.
\end{verbatim}

\noindent
Finally, we define the printf function, and evaluate it on two inputs: 
i)  printf "\%d \%s" "hello Coq" 42, and ii)  printf "\%d \%s" 42 "hello Coq".

\begin{verbatim}
Definition printf s := interp_value (format_string s) "".           

Eval compute in  printf "\%d \%s" "hello Coq"%string 42.
(* Error: The term ""hello Coq"%string" has type "string" 
while it is expected to have type "Z". *)

Eval compute in  printf "\%d \%s" 42 "hello Coq"%string. 
(*  "\0b101010 \hello Coq"%string. The number 
42 is printed in binary *)                            
\end{verbatim}
The first input, printf "\%d \%s" "hello Coq" 42, is type error because 
printf "\%d \%s" returns a value level function whose  type is Z -> string -> string, but 
the type of first argument, "hello Coq", is string which does not unifies with Z,
while second one is successfully printed as string. 

  
  

 
 \subsection{Gallina: The Specification Language}
 \label{sec:gallina}
  The example, type safe printf function, I gave in previous 
  section was encoded in Coq's specification language Gallina. 
  Gallina is a highly expressive specification 
  language for development of mathematical theories and proving the    
  theorems about these  theories; however, writing proofs in Gallina
  is very tedious and cumbersome. Furthermore, It is not suitable for large proof 
  development. In order to ease the proof development, Coq also provides 
  tactics.  The user interacting with Coq theorem prover applies these 
  tactics to build the  Gallina term  which otherwise would  
  be very laborious.
  
 We have written two proofs that addition on natural number is commutative. 
 First proof, \textit{addition\_commutative\_gallina}, is written using 
 Gallina, while the second proof, \textit{addition\_commutative\_tactics}, is written 
 using the tactics.  In general, we write programs directly in Gallina and use tactics 
 to prove properties about the programs. However, there is no fixed set of rules, and tactics 
 can be used to write programs with dependent types (which we have done during this
 formalization).
 
\begin{verbatim}
(* proof written using Gallina *)
Lemma addition_commutative_gallina : forall (n m : nat), n + m = m + n.
refine
      (fix Fn (n : nat) : forall m : nat, n + m = m + n :=
        match n as n0 return (forall m : nat, n0 + m = m + n0) with
        | 0 => fun m : nat => 
           eq_ind_r (fun n0 : nat => m = n0) eq_refl (Nat.add_0_r m)
        | S n' =>
          fun m : nat =>
            eq_ind_r (fun n0 : nat => S n0 = m + S n')
                     (eq_ind_r (fun n0 : nat => S (m + n') = n0) 
                     eq_refl (Nat.add_succ_r m n')) (Fn n' m)
        end).
Qed.

(* proof written using tactics *)
Lemma addition_commutative_tactics : forall (n m : nat), n + m = m + n.
  induction n; intro m; simpl; try omega.
Qed.
\end{verbatim}



 \subsection{Trusting Coq proofs}
 \label{sec:coqproof}
  In general, Coq proofs are nowhere similar to a mathematical 
  proof written by trained mathematician. Also, these proofs 
  are verbose and fairly long, so a 
  very fundamental question is: why should we 
  accept or believe in a proof written in Coq \citep{pollack1998believe}?  Generally, the answer of 
	accepting or trusting Coq proos is two fold:
  i) is the logic (CIC) sound?, and ii) is the implementation correct?
  The logic has already been reviewed by many peers and proved correct 
  using some meta-logic, therefore the answer of our question about trusting Coq proof 
  hinges on the implementation. 
  Coq implementation (written in OCaml)  has two parts, the type checker (small kernel), 
  and tactic language to build the proofs.
  We lay our trust in type checker, because it's small kernel and can be 
  manually inspected. Furthermore, if there
  is a bug in tactic language, which often is the case, then build proof would 
  not pass the type checker.  Also, we can use the publicly available proof 
  checkers written by experts and inspected by many others. In addition, to increase the 
  confidence, there have been 
  efforts to certify type checker \citep{Appel2003}
  \citep{barras1996coq}, verifying meta theory of one proof system 
  in other \citep{10.1007/978-3-319-08970-6_3}, self certificate of 
  theorem prover \citep{10.1007/11814771_17}. However, no system can 
  prove its own consistency (G{\"o}del's second incompleteness theorem), therefore
  trusting human judgement is inevitable.
  
 
    
   ToDo : This is first draft. Finish it and rewrite it. 
\section{Cryptography}
    Cryptography is the science of converting plaintext (understandable to everyone) to ciphertext data. 
    The earliest known usage of cryptography, replacing a symbol by other,  goes back to  ancient 
    Egyptian (Khnumhotep \roman{II} , 1500 BCE); however the purpose of symbol replacing was not to protect
    any sensitive information but to enhance the linguistic appeal. The first known usage of 
    cryptography to conceal the sensitive information goes back Mesopotamians (1500 BCE) where 
    they used it to hide the formula for pottery glaze.  The current form of cryptography mostly 
    developed in 1970. Moreover, the basic principals of modern day cryptography , cryptography hereafter, is based on 
    mathematical principals than vanilla symbol replacement.  Moreover, It has three distinct 
    element:
    \begin{itemize}
    \item data (plaintext data and ciphertext data)
    \item algorithm (encryption algorithm and decryption algorithm)
    \item key (encryption key and decryption key)
    \end{itemize}
    
    Depending on the distinction between the key, encryption key and decryption key, 
    used in the process, the cryptography can be 
    divided into two categories: (i) symmetric key cryptography where the 
    same key is used for encrypting the plaintext data to ciphertext  data, and decrypting 
    the ciphertext data to plaintext data, e.g. \texttt{Data Encryption Scheme}  
    (ii) asymmetric (public) key cryptography where 
    two different keys, public key and private key, are used for encrypting the plaintext 
    data into ciphertext data and ciphertext data to plaintext data, e.g. \texttt{Diffie-Hellman algorithm} 
   Since 1970, the cryptography has evolved, and now, it is not only used for confidentiality, but 
   for integrity, authentication and non-repudiation
   
   The mathematical foundations of cryptography lies in algebraic group
   \begin{itemize}
     \item Group
     \item Field 
     \item Vector 
    \end{itemize}
    
    which we will discuss more in Chapter 7. 
    
    In this thesis, we are concern with public key cryptography. We first describe the \texttt{Diffie-Hellman} \citep{Diffie:2006:NDC:2263321.2269104}
    algorithm, because all the constructions we have used  are based on \texttt{Diffie-Hellman} construction. 
    
    \subsection{Group}
    A group is a set $G$, with a binary operator $f : G x G \rightarrow G$, identity element $e$, and inverse operator $i : G \rightarrow G$ such 
    that the following laws hold: 
    \begin{itemize}
     \item \texttt{Associativity}: $\forall$ a b c $\in$ G, f a (f b c) = f (f a b) c
    \item \texttt{Closure}: $\forall$ a b $\in$ G, f a b $\in$ G
    \item \texttt{Inverse Element}: $\forall$ a $\in$ G, $\exists$ b $\in$ G, such that f a b = f b a = e. b is called inverse of a (inv a) and represented $a^{-1}$
    \item \texttt{Identity}: $\forall$ a $\in$ G, f a e = f e a  = a 
    \end{itemize}
   
    \noindent
    Furthermore, if a group is commutative, i.e. 
    $\forall$ a b $\in$  G, f a b = f b a, then we call it abelian group (in honour of Niels Henrik Abel). 
	    
    
     
     \subsection{Diffie-Hellman Encryption}
     	Diffie-Hellman can be divided into two steps 
     	1. The two communication parties, Alice and Bob, agree with shared public parameter which 
     	   is a finite-cyclic group of order p and generator of the group G := <g> such that $g^{p} = 1 (mod p)$
     	2. After the initial agreement on public parameters between two parties, 
     	    2.1 Alice selects a random element  'a' from the group, computes $g^{a}$ (applying the group operator g ..... (a times), 
     	    and shares the $g^{a}$ with Bob
     	    2.2 Similar Bob selects a random element from 'b' from the group, computes $g^{b}$, and shares it with Alice
     	    2.3 After both parties has exchanged its values, Alice computes $(g^{b})^{a}$ and Bob computes $(g^{a})^{b}$. Since the 
     	    $^$ operator is commutative, both parties hold the value $g^{ab}$ which  is shared secret key. 
     	    2.4 During the whole process, an (polynomial time) attacker would have $g^{a}$ and $g^{b}$ but he can not compute the 
     	   $ g^{ab}$ from these two values assuming that discrete logarithm is hard to compute (There are, off course, other attacks exists).
     
     In nutshell, Diffie-Hellman algorithm computes a shared secret between two communicating parties
     
     \subsection{El-Gamal Encryption Scheme}
     In 1985, Tahir El-Gamal \citep{elgamal1985public} proposed a new encryption system which was ased on Diffie-Hellman algorithm. El-Gamal encryption 
     can has three steps:
     1. Key Generation: The user, Alice, first chooses a finite-cyclic group G of order p and a group group generator g.
        1.2. She randomly selects a element 'x' from set ${1 \cdot (p - 1)}$
        1.3. She computes $h := g^{x} $
        1.4: She publishes <G, g, p, h> 
      2. Encryption: 
       2.1. If any party, say Bob, wants to send a encryption of message m, then he randomly selects an element r (1 <= r < q) from group G, 
       computes a $c1 := g^{r}$ and $c2:= m * h^{r}$, and send it pair (c1, c2) to Alice 
       
      3: Decryption: 
         3.1: In order to get the plaintext message from ciphertext (c1, c2), Alice computes  $c2 * c1^{-x} $
          =$ m * h ^ {r} * g^{-rx} $ = $ m * g ^ {xr} * g^{-rx} = m $
     
     
    
    \subsection{Homomorphic Encryption}
	     Homomorphic encryption  is a encryption scheme which allows us to perform useful operation on 
	     encrypted data without decrypting the data. It was first posed by Rivest, Adleman and Dertouzos in \citep{rivest1978data}: 
	     \begin{displayquote}
	     
	     Consider a small loan company which uses a commercial time-sharing service to store its records.  
	     The loan company’s "data bank" obviously contains sensitive information which should be kept private.  
	     On the other hand, suppose that the information protection techniques employed by the time sharing 
	     service are not considered adequate by the loan company.  In particular, the systems programmers would 
	     presumably have access to the sensitive information.  The loan company therefore decides to encrypt all 
	     of its data kept in the data bank and to maintain a policy of only decrypting data at the home office -- data 
	     will never be decrypted by the time-shared computer.
	     
	     \end{displayquote}  
	     
		A encryption scheme is homomorphic if for any two plaintext $x$ and $y$:
		\begin{displayquote}
		
		Enc(x) $\bigotimes$ Enc(y) = Enc (x $\bigoplus$ y) where 
		$Enc$ is encryption function, $\bigotimes$ is operation on ciphertext, and $\bigoplus$
		is operation on plaintext. These schemes are now know as "somewhat homomorphic". 
		
		\end{displayquote}
				
		These two operators $\bigotimes$ and $\bigoplus$ are very specific. If a cryptosystem that supports an arbitrary 
		function $f$ on ciphertext, then it is called fully homomorphic cryptosystem:
		\begin{displayquote}
		f (Enc(m1), Enc(m2), ..., Enc(mk) = Enc(f(m1, m2, ..., mk)) 
	    \end{displayquote}
		
		\noindent
		The first fully homomorphic encryption system was proposed by Craig Gentry \citep{Gentry:2009:FHE:1834954}; however, 
		in this thesis we are mostly concern with "somewhat homomorphic" encryption, so we are not going to present the details overview 
		of Craig Gentry fully homomorphic construction. From now on, we would be using homomorphic encryption for 
		 "somewhat homomorphic" encryption. 
		 	    
	    
	    
	    
	     
	    Now that, keeping in mind that Homomorphic encryption enables us to perform useful operation on encrypted data, 
	    we will see what kind of homomorphic property is exhibited by the ElGamal method discussed in the previous section. 
	    
	     
	     Now that we have a idea about the Homomorphic encryption, we will see what kind of homomorphic property is 
	     exhibited by ElGamal encryption discusses in the previous section.  Given the public infrastructure (G, p, g, h), 
	     we encrypt two message $m_{1}$ and $m_{2}$:
	     Enc(m1, r1) := ($g^r1$, m1 * $ h^{r1}$) 
	     Enc(m2, r2) := ($g^r2$, m2 * $ h^{r2}$) 
	     
	     If we mutiply these two cipher together, we have  ($g^{r1+ r2}$, m1 * m2 * $ h^{r1 + r2}$) and decrypting this combined ciphertext 
	     would give us m1 * m2. For example, if our end goal is  to achieve multiplication on plaintext, then rather than decrypting each ciphertext individually 
	     and multiplying them, we could simply multiply all the ciphertext together and decrypt the final result. The advantage of this scheme 
	     is that it does not leak the individual values which could some times be requirement of scheme, specifically in electronic voting. 
	     In electronic voting protocols, we do not want to reveal the choices of a individual voter, but it is okay to reveal the final tally.
	     However, the scheme we presented above is not suitable for most of the electronic voting schemes because the final tally 
	     is computed by adding individual choices of each voter while the above scheme is multiplying the choices.  There are 
	     many additive homomorphic encryption schemes, e.g. Benaloh cryptosystem and Paillier cryptosystem. Moreover, we 
	     can modify the ElGamal encryption scheme to make additive. In additive case, it works as 
	     
	      Enc(m1, r1) := ($g^r1$, $g^{m1} $* $ h^{r1}$) 
	      Enc(m2, r2) := ($g^r2$, $g^{m2}$ * $ h^{r2}$) 
	      
	      Multiplying these two ciphers would give us,  ($g^{r1 + r2}$, $g^{m1 + m2} $* $ h^{r1 + r2}$) which would decrypt as 
	      $g^{m1 + m2}$. Using a linear search if the values are small or more efficient search Pohlig–Hellman algorithm. However, the downside 
	      of this scheme is that if the values of m1 + m2 ... mn (assuming n values), then calculating it from $g^{m1 + m2 \cdot mn}$ is 
	      not very practical \citep{10.1007/3-540-69053-0_9}. 
	     
     \subsection{Zero Knowledge Proof}
      In conventional mathematics, a proof of mathematical statement is collection of basic axioms combined according to rules of 
      the system. For example, based on our definition of group given above, we want to prove that: 
      
      forall a $\in$ G, f a a = a $\rightarrow$  a = e
      
      We can prove the statement above by using the rules of  Group (below is a proof in Coq theorem prover)
      
      \begin{verbatim}
      
     
      (* The identity [e] is unique. *)
      Lemma unique_id : forall a, f a a = a -> a = e.
      Proof.
       intros a H.
       rewrite <- (id_r a). (* f a e = e *) 
       rewrite <- (inv_r a). (* f a (inv a) = e *)  
       rewrite <- assoc.
       rewrite H. auto.
       Qed.
      \end{verbatim}
     
     If anyone want to verify our proof, then he would simply check that if the rules are applied correctly. Moreover, these proofs 
     are static in nature. For example, once the prover has produced the proof, then the content of proof is not going to change
     and  there would not be any interaction between prover and verifier. 
     In contrast, zero-knowledge-proof system involves the explicit notion of a interaction between the prover and verifier. Moreover, 
     the goal of the prover is to convince the verifier about the validity of some statement without revealing any information, i.e. 
     the only thing verifier would learn is that if statement is true or false without any other information.  More formally, the 
     zero-knowledge-proof system is 
     
     
     , $Veggy$ with the 
     requirements: 
     \begin{enumerate}
     \item Soundness
     \item Completeness
     \item Zero knowledge
     \end{enumerate}
     
   The goal of the 
     
      
     \subsection{Sigma Protocol}
     
     \subsection{Commitment Schemes}   
        \subsubsection{Discrete Logarithm Based Commitment Scheme}
         Pedersen's Commitment Scheme
    
  		Details from 
  	 \subsection{Sigma Protocol : Efficient Zero Knowledge Proof}
  



\section{Summary}
